{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yearly Trained Model Urban Core Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, re, json, warnings, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterSampler\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"savefig.dpi\"] = 120\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV = \"folder/AllCities_pixels_2003_2020.csv\"\n",
    "OUT_DIR = \"folder/Urban_Core_Year_Wise_Results/outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# strip trailing _YYYY if present; we will append each year later\n",
    "OUT_ROOT = re.sub(r\"_(?:19|20)\\d{2}$\", \"\", OUT_DIR)\n",
    "\n",
    "TARGET = \"HI\"  # predicted ensemble HI\n",
    "NON_FEATURE = [\"row\", \"col\", \"year\", \"lon\", \"city\", \"Residual_HI_1km\"]\n",
    "DROP_EXTRA = [\"LST\", \"WSA\", \"POP\", \"GEO\", \"RAD\", \"DIST2COAST\", \"DPTnorm\", \"IMP\", \"HI_obs_12to1km\"]\n",
    "LIMIT = 368_077\n",
    "N_SHAP = 5000\n",
    "\n",
    "USE_GPU    = True\n",
    "NUM_CORES  = 64\n",
    "N_TRIALS   = 5     # (kept your value)\n",
    "CV_FOLDS   = 3\n",
    "EARLY_STOP = 50\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\":      [300, 500, 800, 1200, 1500],\n",
    "    \"learning_rate\":     [0.01, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\":         [3, 4, 5, 6, 8],\n",
    "    \"min_child_weight\":  [1, 3, 5, 8],\n",
    "    \"subsample\":         [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\":  [0.6, 0.8, 1.0],\n",
    "    \"gamma\":             [0, 0.1, 0.3, 1.0],\n",
    "    \"reg_alpha\":         [0.0, 0.1, 0.5, 1.0],\n",
    "    \"reg_lambda\":        [0.5, 1.0, 2.0, 5.0],\n",
    "}\n",
    "sampler = list(ParameterSampler(param_grid, n_iter=N_TRIALS, random_state=42))\n",
    "\n",
    "# â”€â”€ SHAP export helpers (GPU-safe) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cap numerical library threads inside workers (avoid thread storms)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "# If you want to hide GPUs for the SHAP stage entirely, uncomment:\n",
    "# os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "\n",
    "N_JOBS = int(os.environ.get(\"N_JOBS\", max(1, (os.cpu_count() or 2) - 1)))\n",
    "N_JOBS = min(N_JOBS, 8)\n",
    "PRED_BATCH = 200_000\n",
    "BG_SAMPLE_N = None  # set to e.g. 1000 for a lighter SHAP background\n",
    "\n",
    "_GLOBAL = {\"explainer\": None, \"model\": None, \"feature_names\": None}\n",
    "\n",
    "def _force_cpu_predictor(mdl):\n",
    "    try:\n",
    "        booster = mdl.get_booster()\n",
    "        booster.set_param({'predictor': 'cpu_predictor', 'nthread': 1})\n",
    "    except Exception:\n",
    "        try:\n",
    "            mdl.set_params(predictor='cpu_predictor')\n",
    "            if hasattr(mdl, 'n_jobs'):\n",
    "                mdl.set_params(n_jobs=1)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not force CPU predictor: {e}\")\n",
    "\n",
    "def _predict_cpu_batched(mdl, X_block, batch=PRED_BATCH):\n",
    "    n = len(X_block)\n",
    "    if n <= batch:\n",
    "        return mdl.predict(X_block)\n",
    "    out = []\n",
    "    for i in range(0, n, batch):\n",
    "        out.append(mdl.predict(X_block.iloc[i:i+batch]))\n",
    "    return np.concatenate(out)\n",
    "\n",
    "def _get_explainer(model, X_train, feature_names):\n",
    "    if _GLOBAL[\"explainer\"] is None:\n",
    "        _force_cpu_predictor(model)\n",
    "        _GLOBAL[\"model\"] = model\n",
    "        _GLOBAL[\"feature_names\"] = list(feature_names)\n",
    "        X_bg = X_train\n",
    "        if BG_SAMPLE_N is not None and len(X_train) > BG_SAMPLE_N:\n",
    "            X_bg = X_train.sample(BG_SAMPLE_N, random_state=42)\n",
    "        _GLOBAL[\"explainer\"] = shap.Explainer(model, X_bg, feature_names=feature_names)\n",
    "    return _GLOBAL[\"explainer\"], _GLOBAL[\"model\"], _GLOBAL[\"feature_names\"]\n",
    "\n",
    "def _safe_city_name(city):\n",
    "    return re.sub(r\"[^\\w\\-]+\", \"_\", str(city)).strip(\"_\")\n",
    "\n",
    "def _process_one_group(city, yr, idx, X_block, meta_block, *,\n",
    "                       save_dir: Path, target_name: str,\n",
    "                       model, X_train, feature_names):\n",
    "    try:\n",
    "        explainer, mdl, featnames = _get_explainer(model, X_train, feature_names)\n",
    "        sv_obj = explainer(X_block)\n",
    "        vals   = getattr(sv_obj, \"values\", np.array(sv_obj))\n",
    "        base   = getattr(sv_obj, \"base_values\", explainer.expected_value)\n",
    "        base   = np.atleast_1d(base)\n",
    "        if base.size == 1:\n",
    "            base = np.repeat(base, X_block.shape[0])\n",
    "\n",
    "        shap_df = pd.DataFrame(vals, columns=featnames, index=X_block.index)\n",
    "\n",
    "        meta = meta_block.copy()\n",
    "        meta[\"city\"] = city\n",
    "        meta[\"year\"] = yr\n",
    "        meta[\"HI_pred\"] = _predict_cpu_batched(mdl, X_block)\n",
    "        meta[\"base_value\"] = base\n",
    "\n",
    "        out_df = pd.concat([meta.reset_index(drop=True),\n",
    "                            shap_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        safe_city = _safe_city_name(city)\n",
    "        fp_rows   = save_dir / f\"shap_{safe_city}_{yr}.csv\"\n",
    "        out_df.to_csv(fp_rows, index=False)\n",
    "\n",
    "        summ = pd.DataFrame({\n",
    "            \"feature\": featnames,\n",
    "            \"mean_shap\": np.nanmean(vals, axis=0),\n",
    "            \"mean_abs_shap\": np.nanmean(np.abs(vals), axis=0),\n",
    "            \"n_rows\": X_block.shape[0],\n",
    "            \"city\": city,\n",
    "            \"year\": yr\n",
    "        })\n",
    "        fp_summ = save_dir / f\"shap_summary_{safe_city}_{yr}.csv\"\n",
    "        summ.to_csv(fp_summ, index=False)\n",
    "\n",
    "        del sv_obj, vals, base, shap_df, out_df, summ\n",
    "        gc.collect()\n",
    "        return (city, yr, X_block.shape[0], None)\n",
    "    except Exception as e:\n",
    "        return (city, yr, 0, repr(e))\n",
    "\n",
    "def run_shap_export(df, X, X_train, best, target_name, out_dir_year):\n",
    "    SAVE_DIR = Path(out_dir_year) / \"shap_city_year\"\n",
    "    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_idxed = df.copy()\n",
    "    df_idxed[\"_rowid\"] = np.arange(len(df))\n",
    "    if not {\"city\",\"year\"}.issubset(df_idxed.columns):\n",
    "        raise KeyError(\"df must contain 'city' and 'year' to export by city-year.\")\n",
    "\n",
    "    groups = df_idxed.groupby([\"city\", \"year\"], sort=True)\n",
    "    tasks = []\n",
    "    feature_names = list(X.columns)\n",
    "    meta_keep = [c for c in [\"row\",\"col\",\"lon\",\"lat\"] if c in df.columns] + [target_name]\n",
    "\n",
    "    for (city, yr), g in groups:\n",
    "        idx = g[\"_rowid\"].to_numpy()\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        Xg = X.loc[idx]\n",
    "        if Xg.shape[0] == 0:\n",
    "            continue\n",
    "        meta = df.loc[idx, meta_keep].copy()\n",
    "        meta.rename(columns={target_name: \"HI_true\"}, inplace=True)\n",
    "        tasks.append((city, yr, idx, Xg, meta))\n",
    "\n",
    "    results = Parallel(n_jobs=N_JOBS, backend=\"loky\", prefer=\"processes\")(\n",
    "        delayed(_process_one_group)(\n",
    "            city, yr, idx, Xg, meta,\n",
    "            save_dir=SAVE_DIR,\n",
    "            target_name=target_name,\n",
    "            model=best,\n",
    "            X_train=X_train,\n",
    "            feature_names=feature_names,\n",
    "        )\n",
    "        for (city, yr, idx, Xg, meta) in tqdm(tasks, total=len(tasks), desc=\"SHAP city-year (parallel)\")\n",
    "    )\n",
    "\n",
    "    errs = [(c,y,e) for (c,y,_,e) in results if e]\n",
    "    if errs:\n",
    "        print(\"âš ï¸ Some groups failed:\")\n",
    "        for c,y,e in errs[:10]:\n",
    "            print(f\"  {c}-{y}: {e}\")\n",
    "        if len(errs) > 10:\n",
    "            print(f\"  ... and {len(errs)-10} more\")\n",
    "\n",
    "    print(f\"âœ… Wrote per cityÃ—year SHAP CSVs to: {str(SAVE_DIR)}  (groups={len(tasks)}, n_jobs={N_JOBS})\")\n",
    "\n",
    "# â”€â”€ Load all data once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_all = pd.read_csv(CSV)\n",
    "if TARGET not in df_all.columns:\n",
    "    raise KeyError(f\"Target '{TARGET}' not found in CSV.\")\n",
    "\n",
    "summary_rows = []  # collect per-year metrics\n",
    "\n",
    "# â”€â”€ Train, evaluate, save, SHAP per year â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for YEAR in range(2003, 2021):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Year {YEAR}\")\n",
    "\n",
    "    df = df_all.loc[df_all[\"year\"] == YEAR].copy()\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {YEAR}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        df = df.iloc[:min(LIMIT, len(df))].copy()\n",
    "    df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "    drop_for_X = [TARGET] + NON_FEATURE + DROP_EXTRA\n",
    "    X = df.drop(columns=drop_for_X, errors=\"ignore\")\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    y = df[TARGET].astype(float)\n",
    "\n",
    "    print(f\"TARGET = {TARGET} | YEAR={YEAR} | n_samples={len(y)} | n_features={X.shape[1]}\")\n",
    "    print(f\"Dropped from X: {sorted(set(drop_for_X) & set(df.columns))}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "\n",
    "    # HPO (kept as-is)\n",
    "    best_score  = -np.inf\n",
    "    best_params = None\n",
    "    desc = f\"HPO ({'GPU' if USE_GPU else 'CPU'}, {CV_FOLDS}-fold CV, {YEAR})\"\n",
    "    pbar = tqdm(sampler, total=len(sampler), desc=desc, unit=\"trial\")\n",
    "\n",
    "    for params in pbar:\n",
    "        cv_scores = []\n",
    "        kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            **params,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"gpu_hist\" if USE_GPU else \"hist\",\n",
    "            predictor=\"gpu_predictor\" if USE_GPU else \"auto\",\n",
    "            n_jobs=1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        for tr_idx, val_idx in kf.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=EARLY_STOP,\n",
    "                verbose=False\n",
    "            )\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            cv_scores.append(r2_score(y_val, y_val_pred))\n",
    "\n",
    "        mean_score = float(np.mean(cv_scores))\n",
    "        if mean_score > best_score:\n",
    "            best_score  = mean_score\n",
    "            best_params = params\n",
    "        pbar.set_postfix(best_R2=f\"{best_score:.4f}\", last_R2=f\"{mean_score:.4f}\")\n",
    "    pbar.close()\n",
    "\n",
    "    print(\"\\nBest CV R^2:\", f\"{best_score:.4f}\")\n",
    "    print(\"Best params:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Refit with early stopping\n",
    "    best = xgb.XGBRegressor(\n",
    "        **best_params,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"gpu_hist\" if USE_GPU else \"hist\",\n",
    "        predictor=\"gpu_predictor\" if USE_GPU else \"auto\",\n",
    "        n_jobs=NUM_CORES,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.20, random_state=42\n",
    "    )\n",
    "    best.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose=True\n",
    "    )\n",
    "    best_iter = getattr(best, \"best_iteration\", None)\n",
    "    if best_iter is not None:\n",
    "        print(\"Early-stopping best_iteration:\", best_iter)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = best.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Test R^2 ({TARGET}, {YEAR}): {test_r2:.4f}\")\n",
    "\n",
    "    # Per-year output dir\n",
    "    OUT_DIR_Y = f\"{OUT_ROOT}_{YEAR}\"\n",
    "    os.makedirs(OUT_DIR_Y, exist_ok=True)\n",
    "\n",
    "    # Save metrics & params & model\n",
    "    metrics_row = {\n",
    "        \"year\": YEAR,\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_test\": int(len(X_test)),\n",
    "        \"best_cv_r2\": float(best_score),\n",
    "        \"test_r2\": float(test_r2),\n",
    "        \"best_iteration\": int(best_iter) if best_iter is not None else None\n",
    "    }\n",
    "    pd.DataFrame([metrics_row]).to_csv(os.path.join(OUT_DIR_Y, f\"metrics_{YEAR}.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(OUT_DIR_Y, f\"best_params_{YEAR}.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    model_path = os.path.join(OUT_DIR_Y, f\"model_xgb_{YEAR}.json\")\n",
    "    best.save_model(model_path)\n",
    "    print(f\"Saved model to {model_path}  (load with xgb.XGBRegressor().load_model(path))\")\n",
    "\n",
    "    summary_rows.append(metrics_row)\n",
    "\n",
    "    # Save a small per-city diagnostic (kept from your script, optional)\n",
    "    if \"city\" in df.columns:\n",
    "        test_idx = X_test.index\n",
    "        df_test = df.loc[test_idx].copy()\n",
    "        df_test[\"y_true\"] = y_test.values\n",
    "        df_test[\"y_pred\"] = y_pred\n",
    "        df_test[\"resid\"]  = df_test[\"y_true\"] - df_test[\"y_pred\"]\n",
    "        df_test[\"abs_err\"] = df_test[\"resid\"].abs()\n",
    "\n",
    "        rows = []\n",
    "        for city, g in df_test.groupby(\"city\"):\n",
    "            r2c = r2_score(g[\"y_true\"], g[\"y_pred\"]) if g[\"y_true\"].nunique() >= 2 else np.nan\n",
    "            rmse = float(np.sqrt(np.mean((g[\"y_true\"] - g[\"y_pred\"])**2)))\n",
    "            mae  = float(np.mean(np.abs(g[\"y_true\"] - g[\"y_pred\"])))\n",
    "            rows.append({\"city\": city, \"n\": len(g), \"R2\": r2c, \"RMSE\": rmse, \"MAE\": mae})\n",
    "        city_metrics = pd.DataFrame(rows).sort_values(\"R2\", na_position=\"last\")\n",
    "        city_metrics.to_csv(os.path.join(OUT_DIR_Y, f\"per_city_metrics_{YEAR}.csv\"), index=False)\n",
    "\n",
    "        overall_r2 = r2_score(df_test[\"y_true\"], df_test[\"y_pred\"])\n",
    "        deltas = []\n",
    "        for city, g in df_test.groupby(\"city\"):\n",
    "            mask = df_test[\"city\"] != city\n",
    "            if mask.sum() >= 2 and df_test.loc[mask, \"y_true\"].nunique() >= 2:\n",
    "                r2_wo = r2_score(df_test.loc[mask,\"y_true\"], df_test.loc[mask,\"y_pred\"])\n",
    "                deltas.append({\"city\": city, \"n\": len(g), \"delta_R2_if_removed\": r2_wo - overall_r2})\n",
    "        pd.DataFrame(deltas).sort_values(\"delta_R2_if_removed\", ascending=False)\\\n",
    "            .to_csv(os.path.join(OUT_DIR_Y, f\"city_deltaR2_if_removed_{YEAR}.csv\"), index=False)\n",
    "\n",
    "        cols_to_keep = [\"city\",\"year\",\"lon\",\"lat\",\"y_true\",\"y_pred\",\"resid\",\"abs_err\"]\n",
    "        keep = [c for c in cols_to_keep if c in df_test.columns]\n",
    "        df_test.sort_values(\"abs_err\", ascending=False).loc[:, keep].head(50)\\\n",
    "            .to_csv(os.path.join(OUT_DIR_Y, f\"worst_rows_top50_{YEAR}.csv\"), index=False)\n",
    "\n",
    "    # â”€â”€ SHAP export per city Ã— year (files end with _{YEAR}) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    run_shap_export(df=df, X=X, X_train=X_train, best=best,\n",
    "                    target_name=TARGET, out_dir_year=OUT_DIR_Y)\n",
    "\n",
    "# Save overall summary across years\n",
    "if summary_rows:\n",
    "    pd.DataFrame(summary_rows).to_csv(os.path.join(OUT_ROOT, \"model_performance_summary.csv\"), index=False)\n",
    "    print(f\"\\nðŸ“„ Wrote per-year performance summary to {os.path.join(OUT_ROOT, 'model_performance_summary.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yearly Trained Model SemiUrban Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, re, json, warnings, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterSampler\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams[\"savefig.dpi\"] = 120\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CSV = \"folder/Raw_Data_Urban_Semiurban_Extent/AllCities_pixels_2003_2020.csv\"\n",
    "OUT_DIR = \"folder/Global_XGBoost_Model/Semiurban_Year_Wise_Results/outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# strip trailing _YYYY if present; we will append each year later\n",
    "OUT_ROOT = re.sub(r\"_(?:19|20)\\d{2}$\", \"\", OUT_DIR)\n",
    "\n",
    "TARGET = \"HI\"  # predicted ensemble HI\n",
    "NON_FEATURE = [\"row\", \"col\", \"year\", \"lon\", \"city\", \"Residual_HI_1km\"]\n",
    "DROP_EXTRA = [\"LST\", \"WSA\", \"POP\", \"GEO\", \"RAD\", \"DIST2COAST\", \"DPTnorm\", \"IMP\", \"HI_obs_12to1km\"]\n",
    "LIMIT = 368_077\n",
    "N_SHAP = 5000\n",
    "\n",
    "USE_GPU    = True\n",
    "NUM_CORES  = 64\n",
    "N_TRIALS   = 5     # (kept your value)\n",
    "CV_FOLDS   = 3\n",
    "EARLY_STOP = 50\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\":      [300, 500, 800, 1200, 1500],\n",
    "    \"learning_rate\":     [0.01, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\":         [3, 4, 5, 6, 8],\n",
    "    \"min_child_weight\":  [1, 3, 5, 8],\n",
    "    \"subsample\":         [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\":  [0.6, 0.8, 1.0],\n",
    "    \"gamma\":             [0, 0.1, 0.3, 1.0],\n",
    "    \"reg_alpha\":         [0.0, 0.1, 0.5, 1.0],\n",
    "    \"reg_lambda\":        [0.5, 1.0, 2.0, 5.0],\n",
    "}\n",
    "sampler = list(ParameterSampler(param_grid, n_iter=N_TRIALS, random_state=42))\n",
    "\n",
    "# â”€â”€ SHAP export helpers (GPU-safe) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Cap numerical library threads inside workers (avoid thread storms)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "# If you want to hide GPUs for the SHAP stage entirely, uncomment:\n",
    "# os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "\n",
    "N_JOBS = int(os.environ.get(\"N_JOBS\", max(1, (os.cpu_count() or 2) - 1)))\n",
    "N_JOBS = min(N_JOBS, 8)\n",
    "PRED_BATCH = 200_000\n",
    "BG_SAMPLE_N = None  # set to e.g. 1000 for a lighter SHAP background\n",
    "\n",
    "_GLOBAL = {\"explainer\": None, \"model\": None, \"feature_names\": None}\n",
    "\n",
    "def _force_cpu_predictor(mdl):\n",
    "    try:\n",
    "        booster = mdl.get_booster()\n",
    "        booster.set_param({'predictor': 'cpu_predictor', 'nthread': 1})\n",
    "    except Exception:\n",
    "        try:\n",
    "            mdl.set_params(predictor='cpu_predictor')\n",
    "            if hasattr(mdl, 'n_jobs'):\n",
    "                mdl.set_params(n_jobs=1)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not force CPU predictor: {e}\")\n",
    "\n",
    "def _predict_cpu_batched(mdl, X_block, batch=PRED_BATCH):\n",
    "    n = len(X_block)\n",
    "    if n <= batch:\n",
    "        return mdl.predict(X_block)\n",
    "    out = []\n",
    "    for i in range(0, n, batch):\n",
    "        out.append(mdl.predict(X_block.iloc[i:i+batch]))\n",
    "    return np.concatenate(out)\n",
    "\n",
    "def _get_explainer(model, X_train, feature_names):\n",
    "    if _GLOBAL[\"explainer\"] is None:\n",
    "        _force_cpu_predictor(model)\n",
    "        _GLOBAL[\"model\"] = model\n",
    "        _GLOBAL[\"feature_names\"] = list(feature_names)\n",
    "        X_bg = X_train\n",
    "        if BG_SAMPLE_N is not None and len(X_train) > BG_SAMPLE_N:\n",
    "            X_bg = X_train.sample(BG_SAMPLE_N, random_state=42)\n",
    "        _GLOBAL[\"explainer\"] = shap.Explainer(model, X_bg, feature_names=feature_names)\n",
    "    return _GLOBAL[\"explainer\"], _GLOBAL[\"model\"], _GLOBAL[\"feature_names\"]\n",
    "\n",
    "def _safe_city_name(city):\n",
    "    return re.sub(r\"[^\\w\\-]+\", \"_\", str(city)).strip(\"_\")\n",
    "\n",
    "def _process_one_group(city, yr, idx, X_block, meta_block, *,\n",
    "                       save_dir: Path, target_name: str,\n",
    "                       model, X_train, feature_names):\n",
    "    try:\n",
    "        explainer, mdl, featnames = _get_explainer(model, X_train, feature_names)\n",
    "        sv_obj = explainer(X_block)\n",
    "        vals   = getattr(sv_obj, \"values\", np.array(sv_obj))\n",
    "        base   = getattr(sv_obj, \"base_values\", explainer.expected_value)\n",
    "        base   = np.atleast_1d(base)\n",
    "        if base.size == 1:\n",
    "            base = np.repeat(base, X_block.shape[0])\n",
    "\n",
    "        shap_df = pd.DataFrame(vals, columns=featnames, index=X_block.index)\n",
    "\n",
    "        meta = meta_block.copy()\n",
    "        meta[\"city\"] = city\n",
    "        meta[\"year\"] = yr\n",
    "        meta[\"HI_pred\"] = _predict_cpu_batched(mdl, X_block)\n",
    "        meta[\"base_value\"] = base\n",
    "\n",
    "        out_df = pd.concat([meta.reset_index(drop=True),\n",
    "                            shap_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        safe_city = _safe_city_name(city)\n",
    "        fp_rows   = save_dir / f\"shap_{safe_city}_{yr}.csv\"\n",
    "        out_df.to_csv(fp_rows, index=False)\n",
    "\n",
    "        summ = pd.DataFrame({\n",
    "            \"feature\": featnames,\n",
    "            \"mean_shap\": np.nanmean(vals, axis=0),\n",
    "            \"mean_abs_shap\": np.nanmean(np.abs(vals), axis=0),\n",
    "            \"n_rows\": X_block.shape[0],\n",
    "            \"city\": city,\n",
    "            \"year\": yr\n",
    "        })\n",
    "        fp_summ = save_dir / f\"shap_summary_{safe_city}_{yr}.csv\"\n",
    "        summ.to_csv(fp_summ, index=False)\n",
    "\n",
    "        del sv_obj, vals, base, shap_df, out_df, summ\n",
    "        gc.collect()\n",
    "        return (city, yr, X_block.shape[0], None)\n",
    "    except Exception as e:\n",
    "        return (city, yr, 0, repr(e))\n",
    "\n",
    "def run_shap_export(df, X, X_train, best, target_name, out_dir_year):\n",
    "    SAVE_DIR = Path(out_dir_year) / \"shap_city_year\"\n",
    "    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_idxed = df.copy()\n",
    "    df_idxed[\"_rowid\"] = np.arange(len(df))\n",
    "    if not {\"city\",\"year\"}.issubset(df_idxed.columns):\n",
    "        raise KeyError(\"df must contain 'city' and 'year' to export by city-year.\")\n",
    "\n",
    "    groups = df_idxed.groupby([\"city\", \"year\"], sort=True)\n",
    "    tasks = []\n",
    "    feature_names = list(X.columns)\n",
    "    meta_keep = [c for c in [\"row\",\"col\",\"lon\",\"lat\"] if c in df.columns] + [target_name]\n",
    "\n",
    "    for (city, yr), g in groups:\n",
    "        idx = g[\"_rowid\"].to_numpy()\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        Xg = X.loc[idx]\n",
    "        if Xg.shape[0] == 0:\n",
    "            continue\n",
    "        meta = df.loc[idx, meta_keep].copy()\n",
    "        meta.rename(columns={target_name: \"HI_true\"}, inplace=True)\n",
    "        tasks.append((city, yr, idx, Xg, meta))\n",
    "\n",
    "    results = Parallel(n_jobs=N_JOBS, backend=\"loky\", prefer=\"processes\")(\n",
    "        delayed(_process_one_group)(\n",
    "            city, yr, idx, Xg, meta,\n",
    "            save_dir=SAVE_DIR,\n",
    "            target_name=target_name,\n",
    "            model=best,\n",
    "            X_train=X_train,\n",
    "            feature_names=feature_names,\n",
    "        )\n",
    "        for (city, yr, idx, Xg, meta) in tqdm(tasks, total=len(tasks), desc=\"SHAP city-year (parallel)\")\n",
    "    )\n",
    "\n",
    "    errs = [(c,y,e) for (c,y,_,e) in results if e]\n",
    "    if errs:\n",
    "        print(\"âš ï¸ Some groups failed:\")\n",
    "        for c,y,e in errs[:10]:\n",
    "            print(f\"  {c}-{y}: {e}\")\n",
    "        if len(errs) > 10:\n",
    "            print(f\"  ... and {len(errs)-10} more\")\n",
    "\n",
    "    print(f\"âœ… Wrote per cityÃ—year SHAP CSVs to: {str(SAVE_DIR)}  (groups={len(tasks)}, n_jobs={N_JOBS})\")\n",
    "\n",
    "# â”€â”€ Load all data once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_all = pd.read_csv(CSV)\n",
    "if TARGET not in df_all.columns:\n",
    "    raise KeyError(f\"Target '{TARGET}' not found in CSV.\")\n",
    "\n",
    "summary_rows = []  # collect per-year metrics\n",
    "\n",
    "# â”€â”€ Train, evaluate, save, SHAP per year â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for YEAR in range(2003, 2021):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Year {YEAR}\")\n",
    "\n",
    "    df = df_all.loc[df_all[\"year\"] == YEAR].copy()\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {YEAR}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    if LIMIT is not None:\n",
    "        df = df.iloc[:min(LIMIT, len(df))].copy()\n",
    "    df = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "    drop_for_X = [TARGET] + NON_FEATURE + DROP_EXTRA\n",
    "    X = df.drop(columns=drop_for_X, errors=\"ignore\")\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    y = df[TARGET].astype(float)\n",
    "\n",
    "    print(f\"TARGET = {TARGET} | YEAR={YEAR} | n_samples={len(y)} | n_features={X.shape[1]}\")\n",
    "    print(f\"Dropped from X: {sorted(set(drop_for_X) & set(df.columns))}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "\n",
    "    # HPO (kept as-is)\n",
    "    best_score  = -np.inf\n",
    "    best_params = None\n",
    "    desc = f\"HPO ({'GPU' if USE_GPU else 'CPU'}, {CV_FOLDS}-fold CV, {YEAR})\"\n",
    "    pbar = tqdm(sampler, total=len(sampler), desc=desc, unit=\"trial\")\n",
    "\n",
    "    for params in pbar:\n",
    "        cv_scores = []\n",
    "        kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            **params,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"gpu_hist\" if USE_GPU else \"hist\",\n",
    "            predictor=\"gpu_predictor\" if USE_GPU else \"auto\",\n",
    "            n_jobs=1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        for tr_idx, val_idx in kf.split(X_train):\n",
    "            X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=EARLY_STOP,\n",
    "                verbose=False\n",
    "            )\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            cv_scores.append(r2_score(y_val, y_val_pred))\n",
    "\n",
    "        mean_score = float(np.mean(cv_scores))\n",
    "        if mean_score > best_score:\n",
    "            best_score  = mean_score\n",
    "            best_params = params\n",
    "        pbar.set_postfix(best_R2=f\"{best_score:.4f}\", last_R2=f\"{mean_score:.4f}\")\n",
    "    pbar.close()\n",
    "\n",
    "    print(\"\\nBest CV R^2:\", f\"{best_score:.4f}\")\n",
    "    print(\"Best params:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # Refit with early stopping\n",
    "    best = xgb.XGBRegressor(\n",
    "        **best_params,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"gpu_hist\" if USE_GPU else \"hist\",\n",
    "        predictor=\"gpu_predictor\" if USE_GPU else \"auto\",\n",
    "        n_jobs=NUM_CORES,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.20, random_state=42\n",
    "    )\n",
    "    best.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose=True\n",
    "    )\n",
    "    best_iter = getattr(best, \"best_iteration\", None)\n",
    "    if best_iter is not None:\n",
    "        print(\"Early-stopping best_iteration:\", best_iter)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = best.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Test R^2 ({TARGET}, {YEAR}): {test_r2:.4f}\")\n",
    "\n",
    "    # Per-year output dir\n",
    "    OUT_DIR_Y = f\"{OUT_ROOT}_{YEAR}\"\n",
    "    os.makedirs(OUT_DIR_Y, exist_ok=True)\n",
    "\n",
    "    # Save metrics & params & model\n",
    "    metrics_row = {\n",
    "        \"year\": YEAR,\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_test\": int(len(X_test)),\n",
    "        \"best_cv_r2\": float(best_score),\n",
    "        \"test_r2\": float(test_r2),\n",
    "        \"best_iteration\": int(best_iter) if best_iter is not None else None\n",
    "    }\n",
    "    pd.DataFrame([metrics_row]).to_csv(os.path.join(OUT_DIR_Y, f\"metrics_{YEAR}.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(OUT_DIR_Y, f\"best_params_{YEAR}.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    model_path = os.path.join(OUT_DIR_Y, f\"model_xgb_{YEAR}.json\")\n",
    "    best.save_model(model_path)\n",
    "    print(f\"Saved model to {model_path}  (load with xgb.XGBRegressor().load_model(path))\")\n",
    "\n",
    "    summary_rows.append(metrics_row)\n",
    "\n",
    "    # Save a small per-city diagnostic (kept from your script, optional)\n",
    "    if \"city\" in df.columns:\n",
    "        test_idx = X_test.index\n",
    "        df_test = df.loc[test_idx].copy()\n",
    "        df_test[\"y_true\"] = y_test.values\n",
    "        df_test[\"y_pred\"] = y_pred\n",
    "        df_test[\"resid\"]  = df_test[\"y_true\"] - df_test[\"y_pred\"]\n",
    "        df_test[\"abs_err\"] = df_test[\"resid\"].abs()\n",
    "\n",
    "        rows = []\n",
    "        for city, g in df_test.groupby(\"city\"):\n",
    "            r2c = r2_score(g[\"y_true\"], g[\"y_pred\"]) if g[\"y_true\"].nunique() >= 2 else np.nan\n",
    "            rmse = float(np.sqrt(np.mean((g[\"y_true\"] - g[\"y_pred\"])**2)))\n",
    "            mae  = float(np.mean(np.abs(g[\"y_true\"] - g[\"y_pred\"])))\n",
    "            rows.append({\"city\": city, \"n\": len(g), \"R2\": r2c, \"RMSE\": rmse, \"MAE\": mae})\n",
    "        city_metrics = pd.DataFrame(rows).sort_values(\"R2\", na_position=\"last\")\n",
    "        city_metrics.to_csv(os.path.join(OUT_DIR_Y, f\"per_city_metrics_{YEAR}.csv\"), index=False)\n",
    "\n",
    "        overall_r2 = r2_score(df_test[\"y_true\"], df_test[\"y_pred\"])\n",
    "        deltas = []\n",
    "        for city, g in df_test.groupby(\"city\"):\n",
    "            mask = df_test[\"city\"] != city\n",
    "            if mask.sum() >= 2 and df_test.loc[mask, \"y_true\"].nunique() >= 2:\n",
    "                r2_wo = r2_score(df_test.loc[mask,\"y_true\"], df_test.loc[mask,\"y_pred\"])\n",
    "                deltas.append({\"city\": city, \"n\": len(g), \"delta_R2_if_removed\": r2_wo - overall_r2})\n",
    "        pd.DataFrame(deltas).sort_values(\"delta_R2_if_removed\", ascending=False)\\\n",
    "            .to_csv(os.path.join(OUT_DIR_Y, f\"city_deltaR2_if_removed_{YEAR}.csv\"), index=False)\n",
    "\n",
    "        cols_to_keep = [\"city\",\"year\",\"lon\",\"lat\",\"y_true\",\"y_pred\",\"resid\",\"abs_err\"]\n",
    "        keep = [c for c in cols_to_keep if c in df_test.columns]\n",
    "        df_test.sort_values(\"abs_err\", ascending=False).loc[:, keep].head(50)\\\n",
    "            .to_csv(os.path.join(OUT_DIR_Y, f\"worst_rows_top50_{YEAR}.csv\"), index=False)\n",
    "\n",
    "    # â”€â”€ SHAP export per city Ã— year (files end with _{YEAR}) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    run_shap_export(df=df, X=X, X_train=X_train, best=best,\n",
    "                    target_name=TARGET, out_dir_year=OUT_DIR_Y)\n",
    "\n",
    "# Save overall summary across years\n",
    "if summary_rows:\n",
    "    pd.DataFrame(summary_rows).to_csv(os.path.join(OUT_ROOT, \"model_performance_summary.csv\"), index=False)\n",
    "    print(f\"\\nðŸ“„ Wrote per-year performance summary to {os.path.join(OUT_ROOT, 'model_performance_summary.csv')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
