{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ 0. Set up your paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE      = Path(\"to_folder\")\n",
    "IN_DIR    = BASE / \"folder\"\n",
    "OUT_DIR = BASE / \"HI_India_daymean\"     # where to write the one-fileâ€perâ€year outputs\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€ 1. Loop over each yearâ€™s hourly file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for year in range(2003, 2021):\n",
    "    infile = IN_DIR / f\"HI_hourly_{year}_MAM_IST.nc\"\n",
    "    if not infile.exists():\n",
    "        print(f\"âš ï¸  Missing input for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Open the hourly HI dataset (lazy)\n",
    "    ds = xr.open_dataset(infile)\n",
    "    hi = ds[\"HI_noaa\"]  # dims: time, y, x\n",
    "\n",
    "    # 3. Restrict to the hours 13â€“16 (local/UTC whichever your data are in)\n",
    "    hi_daytime = hi.sel(\n",
    "        time=hi.time.dt.hour.isin([12,13,14,15,16])\n",
    "    )\n",
    "\n",
    "    # 4. Group by calendar day, compute daily mean of those hours\n",
    "    hi_daily = hi_daytime.groupby(\"time.dayofyear\").mean(\"time\")\n",
    "\n",
    "    # 5. Mean across all days in MAM â†’ one 2-D field\n",
    "    hi_daymean = hi_daily.mean(\"dayofyear\", keep_attrs=True)\n",
    "    hi_daymean.name = \"HI_daymean_MAM\"\n",
    "\n",
    "    # 6. Write out to a 2-D NetCDF\n",
    "    outfile = OUT_DIR / f\"HI_daymean_MAM_{year}_IST.nc\"\n",
    "    hi_daymean.to_netcdf(\n",
    "        outfile,\n",
    "        engine=\"netcdf4\",\n",
    "        format=\"NETCDF4\",\n",
    "        encoding={\"HI_daymean_MAM\": {\"zlib\": True, \"complevel\": 4}}\n",
    "    )\n",
    "    print(f\"âœ“ {year}: wrote â†’ {outfile.name}\")\n",
    "\n",
    "    \n",
    "    \n",
    "#!/usr/bin/env python3\n",
    "import xarray as xr\n",
    "import rioxarray       # registers the .rio accessor\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ 0. Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "NC_DIR    = BASE / \"folder_path\"       # your yearly .nc inputs\n",
    "SHAPEFILE = Path(\n",
    "    \"folder/india_simple_outline.shp\"\n",
    ")\n",
    "OUT_DIR   = BASE / \"HI_NOAA_MAX_12_16\" / \"HI\"  # where to write the .tifs\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ 1. Read India boundary once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gdf = gpd.read_file(SHAPEFILE).to_crs(\"EPSG:4326\")\n",
    "\n",
    "# â”€â”€ 2. Loop over years â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for year in range(2003, 2021):\n",
    "    nc_file = NC_DIR / f\"HI_daymean_MAM_{year}_IST.nc\"\n",
    "    if not nc_file.exists():\n",
    "        print(f\"âš ï¸  Missing NetCDF for {year}, skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"â–¶ Processing {year}â€¦\")\n",
    "\n",
    "    # 2a) Open the NetCDF and grab the DataArray\n",
    "    da = xr.open_dataset(nc_file)[\"HI_daymean_MAM\"]\n",
    "\n",
    "    # 2b) Give it a CRS and clip to the India polygon\n",
    "    da_clipped = (\n",
    "        da\n",
    "        .rio.write_crs(\"EPSG:4326\")\n",
    "        .rio.clip(gdf.geometry, gdf.crs, drop=True)\n",
    "    )\n",
    "\n",
    "    # 2c) Build the output path\n",
    "    out_tif = OUT_DIR / f\"HI_daymean_{year}_MAM_12-16.tif\"\n",
    "\n",
    "    # 2d) Export as a singleâ€band LZWâ€compressed GeoTIFF\n",
    "    da_clipped.rio.to_raster(\n",
    "        out_tif,\n",
    "        driver=\"GTiff\",\n",
    "        compress=\"LZW\",\n",
    "        dtype=\"float32\",\n",
    "        nodata=-9999\n",
    "    )\n",
    "\n",
    "    print(f\"   âœ“ saved {out_tif.name}\")\n",
    "\n",
    "print(\"\\nAll done â€” your GeoTIFFs are in:\", OUT_DIR)\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "\n",
    "# 1. Input & output directories\n",
    "src_dir = \"folder_LC_outputs\"\n",
    "dst_dir = \"folder/LC_mosaics\"\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# 2. Regex to extract year & part\n",
    "pattern = re.compile(r\"landcover_(\\d{4})_part([1-4])-.*\\.tif$\")\n",
    "\n",
    "\n",
    "# 3. Collect and group files\n",
    "groups = {}\n",
    "for fp in glob.glob(os.path.join(src_dir, \"*.tif\")):\n",
    "    fn = os.path.basename(fp)\n",
    "    m = pattern.match(fn)\n",
    "    if not m:\n",
    "        continue\n",
    "    year, part = m.groups()\n",
    "    groups.setdefault((year, part), []).append(fp)\n",
    "\n",
    "# 4. Mosaic each group\n",
    "for (year, part), tif_list in groups.items():\n",
    "    print(f\"Mosaicking Year={year} Part={part} ({len(tif_list)} tiles)â€¦\")\n",
    "    src_files = [rasterio.open(p) for p in tif_list]\n",
    "    mosaic, out_trans = merge(src_files)\n",
    "    \n",
    "    # 5. Update metadata\n",
    "    out_meta = src_files[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\":  mosaic.shape[2],\n",
    "        \"transform\": out_trans\n",
    "    })\n",
    "    \n",
    "    # 6. Write mosaic\n",
    "    out_path = os.path.join(\n",
    "        dst_dir,\n",
    "        f\"landcover_{year}_part{part}_mosaic.tif\"\n",
    "    )\n",
    "    with rasterio.open(out_path, \"w\", **out_meta) as dst:\n",
    "        dst.write(mosaic)\n",
    "    \n",
    "    # 7. Close sources\n",
    "    for src in src_files:\n",
    "        src.close()\n",
    "\n",
    "print(\"All done! Check\", dst_dir)\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Year-by-year ensemble downscaling of Heat Index:\n",
    "- RandomForest for central HI (mean loss)\n",
    "- GradientBoostingRegressor (quantile loss) for high-HI tail\n",
    "\n",
    "Predictors: LST + WSA + POP + GEO + RAD + DIST2COAST + DPTnorm + IMP\n",
    "\n",
    "For each year (2003â€“2020):\n",
    "  1. Sample SAMPLE_FRAC of all valid India pixels\n",
    "  2. Stack predictors & target\n",
    "  3. Train/test split\n",
    "  4. Oversample high-HI tail for quantile model\n",
    "  5. Fit RF (mean) and GBR (quantile)\n",
    "  6. Combine predictions: use GBR where it exceeds RF, otherwise RF\n",
    "  7. Predict and write full-grid map\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Paths & settings\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "RF_OUTDIR       = \"folder/Ensemble_peryear\"\n",
    "INDIA_SHP       = os.path.join(ROOT, \"india_simple_outline/india_simple_outline.shp\")\n",
    "YEARS           = list(range(2003, 2021))\n",
    "SAMPLE_FRAC     = 1.0       # sample fraction\n",
    "RF_N_EST        = 150       # RF trees\n",
    "GBR_N_EST       = 300       # GBR trees\n",
    "GBR_LR          = 0.05      # learning rate for quantile\n",
    "GBR_MAX_DEPTH   = 5\n",
    "RND             = 7\n",
    "QUANTILE_ALPHA  = 0.9       # focus on 90th percentile\n",
    "EXTREME_FRAC    = 0.3       # fraction of training for extremes\n",
    "\n",
    "random.seed(RND)\n",
    "np.random.seed(RND)\n",
    "os.makedirs(RF_OUTDIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Build 1â€‰km template & mask\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "india_gdf = gpd.read_file(INDIA_SHP).to_crs(\"EPSG:4326\")\n",
    "poly = india_gdf.geometry.unary_union\n",
    "if isinstance(poly, Polygon):\n",
    "    poly = MultiPolygon([poly])\n",
    "\n",
    "def open_1km_template():\n",
    "    sample = os.path.join(ROOT, \"LST/LST_INDIA_2007_MAM.tif\")\n",
    "    with rasterio.open(sample) as src:\n",
    "        prof = src.profile\n",
    "        h, w = src.height, src.width\n",
    "        ys = np.linspace(src.bounds.top, src.bounds.bottom, h)\n",
    "        xs = np.linspace(src.bounds.left, src.bounds.right, w)\n",
    "    return (h, w), prof, ys, xs\n",
    "\n",
    "(grid_h, grid_w), grid_profile, y_coords, x_coords = open_1km_template()\n",
    "\n",
    "india_mask = geometry_mask([\n",
    "    poly\n",
    "], invert=True,\n",
    "    transform=grid_profile['transform'],\n",
    "    out_shape=(grid_h, grid_w)\n",
    ")\n",
    "lat_grid = np.repeat(y_coords.reshape(-1,1), grid_w, axis=1)\n",
    "lon_grid = np.repeat(x_coords.reshape(1,-1), grid_h, axis=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Raster reader & static predictors\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def read_raster(path, match_profile=None):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read(1).astype(np.float32)\n",
    "        if match_profile:\n",
    "            aligned = (\n",
    "                src.transform == match_profile['transform'] and\n",
    "                src.crs      == match_profile['crs'] and\n",
    "                src.width    == match_profile['width'] and\n",
    "                src.height   == match_profile['height']\n",
    "            )\n",
    "            if not aligned:\n",
    "                dst = np.full((match_profile['height'], match_profile['width']),\n",
    "                              np.nan, np.float32)\n",
    "                reproject(\n",
    "                    source=arr,\n",
    "                    destination=dst,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=match_profile['transform'],\n",
    "                    dst_crs=match_profile['crs'],\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                return dst\n",
    "        return arr\n",
    "\n",
    "# static layers\n",
    "geo  = read_raster(os.path.join(ROOT, \"GEO_INDIA_1km.tif\"), grid_profile)\n",
    "dist = read_raster(os.path.join(ROOT, \"distance_tocoast/India_Coast_Distance_km.tif\"), grid_profile)\n",
    "geo[~india_mask]  = np.nan\n",
    "dist[~india_mask] = np.nan\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Loop over years\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for yr in tqdm(YEARS, desc=\"Processing years\"):\n",
    "    print(f\"\\n--- Year {yr} ---\")\n",
    "    # load annual rasters\n",
    "    lst     = read_raster(f\"{ROOT}/LST/LST_INDIA_{yr}_MAM.tif\", grid_profile)\n",
    "    wsa     = read_raster(f\"{ROOT}/WSA/WSA_INDIA_{yr}_MAM.tif\", grid_profile)\n",
    "    pop     = read_raster(f\"{ROOT}/POP_outputs/POP_INDIA_{yr}.tif\", grid_profile)\n",
    "    rad     = read_raster(f\"{ROOT}/ERA_RAD_outputs_norm/RAD_INDIA_{yr}_MAM.tif\", grid_profile)\n",
    "    dptnorm = read_raster(f\"{ROOT}/ERA_DPTnorm_outputs/DPTNORM_INDIA_{yr}_MAM.tif\", grid_profile)\n",
    "    imp     = read_raster(f\"{ROOT}/LC_final/landcover_{yr}.tif\", grid_profile)\n",
    "    hi = read_raster(  f\"/home/udit/Desktop/Ongoing_Projects/Ashish/Heat_Index_Work/HI_NOAA_MAX_12_16/HI/HI_daymean_{yr}_MAM_12-16.tif\", grid_profile)\n",
    "\n",
    "    # valid mask & sample\n",
    "    valid = (\n",
    "        india_mask & np.isfinite(lst) & np.isfinite(wsa)\n",
    "        & np.isfinite(pop) & np.isfinite(geo)\n",
    "        & np.isfinite(rad) & np.isfinite(dist)\n",
    "        & np.isfinite(dptnorm) & np.isfinite(imp)\n",
    "        & np.isfinite(hi)\n",
    "    )\n",
    "    idx = np.where(valid.ravel())[0]\n",
    "    if idx.size == 0:\n",
    "        print(\" No valid pixels; skipping.\")\n",
    "        continue\n",
    "    nsel = int(SAMPLE_FRAC * idx.size)\n",
    "    print(f\" Sampled {nsel} valid pixels\")\n",
    "    sel  = np.random.choice(idx, nsel, replace=False)\n",
    "\n",
    "    # stack features & target\n",
    "    X = np.column_stack([\n",
    "        np.full(nsel, yr, dtype=np.int16),\n",
    "        lat_grid.ravel()[sel], lon_grid.ravel()[sel],\n",
    "        lst.ravel()[sel], wsa.ravel()[sel],\n",
    "        pop.ravel()[sel], geo.ravel()[sel],\n",
    "        rad.ravel()[sel], dist.ravel()[sel],\n",
    "        dptnorm.ravel()[sel], imp.ravel()[sel]\n",
    "    ]).astype(np.float32)\n",
    "    y = hi.ravel()[sel].astype(np.float32)\n",
    "\n",
    "    # train/test split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=RND\n",
    "    )\n",
    "    print(f\" Train/test split: {X_tr.shape[0]} train, {X_te.shape[0]} test\")\n",
    "\n",
    "    # RF for mean HI\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=RF_N_EST,\n",
    "        max_features='sqrt',\n",
    "        max_depth=None,\n",
    "        random_state=RND,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    rf_pred_te = rf.predict(X_te)\n",
    "    rf_mae = mean_absolute_error(y_te, rf_pred_te)\n",
    "    print(f\" RF hold-out MAE: {rf_mae:.3f} Â°C\")\n",
    "\n",
    "    # oversample extremes & quantile GBR\n",
    "    thr = np.quantile(y_tr, QUANTILE_ALPHA)\n",
    "    ext_idx    = np.where(y_tr > thr)[0]\n",
    "    nonext_idx = np.where(y_tr <= thr)[0]\n",
    "    n_ext      = int(EXTREME_FRAC * len(y_tr))\n",
    "    n_non      = len(y_tr) - n_ext\n",
    "    sel_ext    = np.random.choice(ext_idx,    n_ext,   replace=(n_ext>len(ext_idx)))\n",
    "    sel_non    = np.random.choice(nonext_idx, n_non, replace=False)\n",
    "    print(f\" Extreme threshold ({QUANTILE_ALPHA*100:.0f}th percentile): {thr:.2f} Â°C\")\n",
    "    print(f\" Oversampling: {len(ext_idx)} available extremes -> {n_ext} used, {n_non} non-extreme\")\n",
    "    idx_bal    = np.concatenate([sel_ext, sel_non])\n",
    "    Xb, yb     = X_tr[idx_bal], y_tr[idx_bal]\n",
    "\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        loss='quantile',\n",
    "        alpha=QUANTILE_ALPHA,\n",
    "        learning_rate=GBR_LR,\n",
    "        max_depth=GBR_MAX_DEPTH,\n",
    "        n_estimators=1000,            # an upper bound\n",
    "        n_iter_no_change=10,          # stop if no improvement\n",
    "        validation_fraction=0.1,      # hold out 10% of your Xb,yb\n",
    "        random_state=RND\n",
    "    )\n",
    "\n",
    "    gbr.fit(Xb, yb)\n",
    "    gbr_pred_te = gbr.predict(X_te)\n",
    "    gbr_mae = mean_absolute_error(y_te, gbr_pred_te)\n",
    "    print(f\" GBR (quantile) hold-out MAE: {gbr_mae:.3f} Â°C\")\n",
    "\n",
    "    # ensemble combine\n",
    "    pred_te = np.where(gbr_pred_te > rf_pred_te, gbr_pred_te, rf_pred_te)\n",
    "    ens_mae = mean_absolute_error(y_te, pred_te)\n",
    "    print(f\" Ensemble hold-out MAE: {ens_mae:.3f} Â°C\")\n",
    "\n",
    "    # full-grid predict\n",
    "    Npix = grid_h * grid_w\n",
    "    Xg = np.column_stack([\n",
    "        np.full(Npix, yr, dtype=np.int16),\n",
    "        lat_grid.ravel(), lon_grid.ravel(),\n",
    "        lst.ravel(), wsa.ravel(), pop.ravel(), geo.ravel(),\n",
    "        rad.ravel(), dist.ravel(), dptnorm.ravel(), imp.ravel()\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    # flatten valid mask and prepare output container\n",
    "    valid_flat = valid.ravel()\n",
    "    pred_flat = np.full(Npix, np.nan, dtype=np.float32)\n",
    "\n",
    "    # only predict on valid pixels\n",
    "    Xg_valid = Xg[valid_flat]\n",
    "    rf_valid = rf.predict(Xg_valid)\n",
    "    gbr_valid = gbr.predict(Xg_valid)\n",
    "    ens_valid = np.where(gbr_valid > rf_valid, gbr_valid, rf_valid)\n",
    "\n",
    "    # scatter predictions back into full grid\n",
    "    pred_flat[valid_flat] = ens_valid\n",
    "    pred = pred_flat.reshape(grid_h, grid_w)\n",
    "\n",
    "    # write output TIFF\n",
    "    out_fp = os.path.join(RF_OUTDIR, f\"HI_ensemble_{yr}.tif\")\n",
    "    prof   = grid_profile.copy()\n",
    "    prof.update(dtype='float32', count=1, compress='LZW', nodata=np.nan)\n",
    "    with rasterio.open(out_fp, 'w', **prof) as dst:\n",
    "        dst.write(pred, 1)\n",
    "\n",
    "    # cleanup\n",
    "    del lst, wsa, pop, rad, dptnorm, imp, hi, X, y, X_tr, X_te, y_tr, y_te, rf, gbr, Xg, pred\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nðŸŽ‰ All years done with ensemble!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
